#!/bin/bash
#

# Mandatory
RDMA_HOST=node-05

# Optional, highly discouraged, please use the beaker settings to
# control this instead if at all possible, but if not...
#
# NOTE: This does not overwrite or remove any options, it merges
# what you put here into GRUB_CMDLINE_LINUX in /etc/default/grub.
# Removal of unwanted options from that variable requires removing
# those items from the beaker config for this host

# HOST_GRUB_OPTIONS="intel_iommu=on iommu=on"

# Mandatory and must include all rdma fabrics this machine is on
# from the list of possible fabrics: ib0 ib1 opa0 opa1 roce iw
# Format is required to be a bash variable array, so something like
# HOST_FABRICS=(ib0 ib1 roce) is valid
HOST_FABRICS=(ib0 roce)

# Not exactly mandatory, but necessary to make it easy to rename interfaces
# to expected names.  This clears out all of the enps6f1 type names that
# the system creates on install so we can use our simple, descriptive
# names like lom_1, lom_2, lab-bridge0, etc.
Clear_Default_Interfaces

##  Create Test Lab Network Interface Config Files
#
# NOTE: For readability, multiple ways of setting up the test lab network
#       are shown uncommented.  You should comment out the method you
#       aren't going to use on this machine.
#
# This is the section where we normally add the various interfaces the
# system uses.  If you use Clear_Default_Interfaces, then you need to
# create lines for all interfaces, both motherboard and add-in card alike.
#
# Typical motherboard ethernet port connected to lab network

Create_Interface lom_1 Ethernet yes hwaddr d4:f5:ef:0e:7e:e4 dhcp defroute

# If you want to have a bridge interface so you can have virtual machines
# bridged directly to the test lab Ethernet network, then first create
# the bridge, then slave your motherboard Ethernet to it, like so:

#Create_Interface lab-bridge0 Bridge yes dhcp defroute mac <MAC address of lom_1> stp no priority 32768
#Create_Interface lom_1 Ethernet yes hwaddr <MAC address> bridge lab-bridge0

# Typical extra motherboard ethernet ports that are not in use

Create_Interface lom_2 Ethernet no hwaddr d4:f5:ef:0e:7e:e5
Create_Interface lom_3 Ethernet no hwaddr d4:f5:ef:0e:7e:e6
Create_Interface lom_4 Ethernet no hwaddr d4:f5:ef:0e:7e:e7

# Please note that in order for the en command to show you your port state,
# the interfaces need to be named lab-bridge0 and lom_1.  See the definition
# of the en() command in the bashrc file, it shows the complete list of
# Ethernet names that it searches for in the ip l output.  Also, please
# note that the bridge mac and the lom_1 hwaddr should be the same, as that's
# required in order to get proper dhcp in the latest Fedora (otherwise
# Fedora creates a unique MAC address for the bridge and it won't be
# registered with the test lab DHCP server and you'll get a random DHCP
# address).
#
# See the Create_Interface function in rdma-functions.sh for a complete
# description, or on a machine where you have sourced the rdma-functions.sh
# file (such as an rdma-* machine), you can just type Create_Interface and
# hit return and get back the usage information
#
##

## Create RDMA Network Interface Files
#
# Create_Rdma_Interfaces is a wrapper for Create_Interface that knows about
# all of the P_Keys, vlans, etc. on a given fabric and will call
# Create_Interface multiple times, once for each P_Key or vlan on the
# given fabric.  This is the normal way to create a set of RDMA interface
# files.
#
# The basic format is:
#
# Create_Rdma_Interfaces <devicename> <fabric> <identifier> <dhcp|ip address final octet> [defroute]
# The devicename is usually descriptive of the hardware, like mlx4, cxgb4, etc
# The fabric is one of the valid fabrics: ib0 ib1 opa0 opa1 roce iw
# The identifier is either the MAC address for Ethernet devices, or the IPoIB
#   link hwaddr for other devices
# The next option is either the keyword dhcp to get your address via dhcp or
#   you can use static IP addressing, in which case you need to know your
#   assigned IP address, such as 172.31.0.254 for your address on ib0, then
#   you need to put *just* the number 254 here and the wrapper will create
#   all of the static IP address mappings for all of your P_Keys or vlans
#   using the known subnet of each P_Key/vlan combined with this final number
# Finally, you may pass the keyword defroute.  This is only intended for
#   use with machines being used to test PXE boot on RDMA hardware.  In that
#   case, you need to unplug the test lab Ethernet on that machine, set it
#   to PXE boot from the RDMA hardware, set the tftp images on rdma-master
#   manually, set rdma-master to MASQUERADE your client to the test lab
#   network, then PXE boot from the RDMA hardware and you should be able
#   to access test lab resource by routing through rdma-master.
#
# Here is a common set of RDMA interfaces as examples

Create_Rdma_Interfaces mlx5a ib0 00:00:07:ab:fe:80:00:00:00:00:00:00:b8:ce:f6:03:00:2c:5f:de dhcp
Create_Rdma_Interfaces mlx5b ib0 00:00:08:58:fe:80:00:00:00:00:00:00:b8:ce:f6:03:00:2c:5f:e2 dhcp
Create_Interface mlx5a_roce_off Ethernet no hwaddr b8:ce:f6:2c:5f:df
Create_Interface mlx5b_roce_off Ethernet no hwaddr b8:ce:f6:2c:5f:e3
Create_Rdma_Interfaces bnxt roce 00:0a:f7:31:49:50 dhcp
Create_Interface bnxt_off Ethernet no hwaddr 00:0a:f7:31:49:51

# This is optional.  It simply adds a multicast route to a given interface.
# You can override this with most multicast tools by telling them which
# interface to use, so this really only helps in the case that you want
# to run those tools without needing to specify which interface to use.
Create_Multicast_Route bnxt0_roce

#
# See the usage output for both Create_Interface and Create_Rdma_Interfaces
# for more information
#
##

## Information needed for dhcp to work
#
# Mandatory, and requires that RDMA_HOST conform to our allowed hostnames
Create_Fixed_Addresses ${HOST_FABRICS[*]}
# Mandatory if using any RDMA over Ethernet devices
HARDWARE=(00:0a:f7:31:49:50)
# Mandatory if using any IB or OPA devices
GIDS=(b8:ce:f6:03:00:2c:5f:de b8:ce:f6:03:00:2c:5f:e2)
#
##

# Optional.  Used to put free form configuration settings into files.
# Requires that the file in question honor the shell script standard
# notation that all lines starting with a # character are comments.
# Will then insert your specific option settings between two comment
# lines marked with the description element.  For example, this line:
#Set_Config_Options /etc/modprobe.d/mlx4.conf "module options for $HOST" "options mlx4_en pfctx=0x28 pfcrx=0x28"
# Would add these lines (minus the leading '# ') to the file
# /etc/modprobe.d/mlx4.conf:
#
# # module options for <hostname>
# options mlx4_en pfctx=0x28 pfcrx=0x28
# # end module options for <hostname>
#
# In this way, successive runs of the rdma-setup.sh script can avoid adding
# the same config option multiple times and instead will overwrite any
# previous config modifications on subsequent runs

# Mostly mandatory, create NFS mountpoints and entries in /etc/fstab for test
# lab provided NFS services, which allows easy access to things like the
# brew buildroot for installing packages.  Also creates all the mountpoints
# for rdma-storage-* servers on all machines in the rdma cluster
Setup_Nfs_Client_Mounts

##  Server Specific Settings
# Various optional server setup routines.  Many of these are specific to
# a given machine or machines and will refuse to run on others.

# Deprecated.  We are using the switch SMs now.  Left here in case we
# switch back to running opensm
#Setup_Opensm

# Setup opafm.  Only used on rdma-master
#Setup_Opafm

# Setup for being a dhcp server, only used on rdma-master and rdma-storage-01
#Setup_Dhcp_Server

# Setup_Nfs_Server - Optional.  Only good on already defined NFSoRDMA
#   servers.  Requires that this host be enumerated in the arrays in the
#   nfs-mounts file
#Setup_Nfs_Server

# Setup_Nfs_Exports - Optional, but must be after Setup_Nfs_Server.
#   However, any nfs exports defined here should either be created in this
#   script (such as by calling lvm commands to create the lvm devices), and
#   then mkfs to create the filesystems, and then by adding the filesystems
#   to /etc/fstab as mount points under /srv/NFSoRDMA, or they need to be
#   pre-existing mount points preserved during the install.  For the sake
#   of example, including code needed to create devices, add mountpoints,
#   and then the Setup_Nfs_Exports call.
#
# for vg in NFSoRDMAv{3,4.0,4.1}_xfs; do
# 	if [ ! -e /dev/srv_vg/$vg ]; then
# 		lvm lvcreate -L 250G -Z n -n $vg srv_vg
# 		sleep 1
# 		umount -f /dev/srv_vg/$vg
# 		dd if=/dev/zero of=/dev/srv_vg/$vg bs=1k count=4
# 		mkfs.xfs -f /dev/srv_vg/$vg
# 	fi
# done
# for vg in NFSoRDMAv{3,4.0,4.1}_ext4; do
#	if [ ! -e /dev/srv_vg/$vg ]; then
#		lvm lvcreate -L 250G -Z n -n $vg srv_vg
#		sleep 1
#		umount -f /dev/srv_vg/$vg
#		dd if=/dev/zero of=/dev/srv_vg/$vg bs=1k count=4
#		mkfs.ext4 /dev/srv_vg/$vg
#	fi
#done
#Set_Config_Options /etc/fstab "local nfs export mounts" "/dev/srv_vg/NFSoRDMAv3_
#xfs	/srv/NFSoRDMA/v3-xfs	xfs	defaults 0 0
#/dev/srv_vg/NFSoRDMAv3_ext4	/srv/NFSoRDMA/v3-ext4	ext4	defaults 0 0
#/dev/srv_vg/NFSoRDMAv4.0_xfs	/srv/NFSoRDMA/v4.0-xfs	xfs	defaults 0 0
#/dev/srv_vg/NFSoRDMAv4.0_ext4	/srv/NFSoRDMA/v4.0-ext4	ext4	defaults 0 0
#/dev/srv_vg/NFSoRDMAv4.1_xfs	/srv/NFSoRDMA/v4.1-xfs	xfs	defaults 0 0
#/dev/srv_vg/NFSoRDMAv4.1_ext4	/srv/NFSoRDMA/v4.1-ext4	ext4	defaults 0 0"
#Setup_Nfs_Exports v3-xfs v3-ext4 v4.0-xfs v4.0-ext4 v4.1-xfs v4.1-ext4

# Enable_Tgtd - Optional, required before calling Config_Tgtd.  Any servers
#   wishing to export iSER or SRP devices need to call this and also need
#   to have their entries in the wwns file filled out.
# Config_Tgtd - Optional.  Configure a block device to be used as either
#   an iSER or SRP share.  The block device must already exist.  It
#   should not be a mountpoint as that can cause corruption.  It does
#   not need to have any filesystem on the device as the client is free
#   to use the block device in any way they wish, including with any
#   filesystem they wish.  For the sake of example, including all the
#   code necessary to create the block devices and configure them
#   for client use.
#
#   NOTE: This is not sufficient for the client to use the devices.  Once
#         they are created here, the block device will be configured.
#         However, for actual access, we need to have an ACL that allows
#         the client to use the device.  Since we don't have that
#         information, the client must use Setup_Iser_Client_Mounts
#         and Setup_Srp_Client_Mounts after their own install is
#         completed before the device is fully ready for use.
#
# We must call Enable_Tgtd before calling Config_Tgtd
#Enable_Tgtd
#for i in dev-{00..16} qe-{02..15} perf-{00..05} virt-{00..07}; do
#	if [ ! -e /dev/srv_vg/iser-$i ]; then
#		lvm lvcreate -L 40G -Z n -n iser-$i srv_vg
#		sleep 1
#		umount -f /dev/srv_vg/iser-$i
#		dd if=/dev/zero of=/dev/srv_vg/iser-$i bs=1k count=4
#	fi
#	if [ ! -e /dev/srv_vg/srp-$i ]; then
#		lvm lvcreate -L 40G -Z n -n srp-$i srv_vg
#		sleep 1
#		umount -f /dev/srv_vg/srp-$i
#		dd if=/dev/zero of=/dev/srv_vg/srp-$i bs=1k count=4
#	fi
#	Config_Tgtd $i /dev/srv_vg/iser-$i iser
#	Config_Tgtd $i /dev/srv_vg/srp-$i srp
#done
#
##
